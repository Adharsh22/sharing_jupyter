{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATADIR = \"F:\\PetImages\"\n",
    "\n",
    "CATEGORIES = [\"Dog1\", \"Cat1\"]\n",
    "\n",
    "for category in CATEGORIES:  # do dogs and cats\n",
    "    path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "    for img in os.listdir(path):  # iterate over each image per dogs and cats\n",
    "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "        IMG_SIZE = 100\n",
    "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "        plt.imshow(new_array, cmap='gray')  # graph it\n",
    "        plt.show()  # display!\n",
    "\n",
    "        break  # we just want one for now so break\n",
    "    break  #...and one more!\n",
    "#print(img_array)\n",
    "#print(img_array.shape)\n",
    "#IMG_SIZE = 150\n",
    "#new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "#plt.imshow(new_array, cmap='gray')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 486/486 [00:00<00:00, 491.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 486/486 [00:00<00:00, 588.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:  # do dogs and cats\n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                training_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "            #except OSError as e:\n",
    "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "            #except Exception as e:\n",
    "            #    print(\"general exception\", e, os.path.join(path,img))\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)\n",
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 76]\n",
      "   [ 78]\n",
      "   [ 76]\n",
      "   ...\n",
      "   [224]\n",
      "   [223]\n",
      "   [225]]\n",
      "\n",
      "  [[ 64]\n",
      "   [ 62]\n",
      "   [ 68]\n",
      "   ...\n",
      "   [223]\n",
      "   [223]\n",
      "   [221]]\n",
      "\n",
      "  [[ 58]\n",
      "   [ 67]\n",
      "   [ 80]\n",
      "   ...\n",
      "   [216]\n",
      "   [223]\n",
      "   [222]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 73]\n",
      "   [ 69]\n",
      "   [ 57]\n",
      "   ...\n",
      "   [241]\n",
      "   [241]\n",
      "   [241]]\n",
      "\n",
      "  [[ 90]\n",
      "   [ 65]\n",
      "   [ 51]\n",
      "   ...\n",
      "   [241]\n",
      "   [241]\n",
      "   [241]]\n",
      "\n",
      "  [[ 60]\n",
      "   [ 69]\n",
      "   [ 69]\n",
      "   ...\n",
      "   [241]\n",
      "   [241]\n",
      "   [241]]]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "print(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "#We can always load it in to our current script, or a totally new one by doing:\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-0-dense-1549450564\n",
      "Train on 679 samples, validate on 292 samples\n",
      "Epoch 1/10\n",
      "679/679 [==============================] - ETA: 24s - loss: 0.6910 - acc: 0.53 - ETA: 20s - loss: 0.6967 - acc: 0.51 - ETA: 18s - loss: 0.6944 - acc: 0.53 - ETA: 17s - loss: 0.6906 - acc: 0.55 - ETA: 16s - loss: 0.6915 - acc: 0.53 - ETA: 15s - loss: 0.6925 - acc: 0.52 - ETA: 14s - loss: 0.6943 - acc: 0.50 - ETA: 13s - loss: 0.6938 - acc: 0.50 - ETA: 12s - loss: 0.6941 - acc: 0.50 - ETA: 11s - loss: 0.6937 - acc: 0.51 - ETA: 10s - loss: 0.6945 - acc: 0.50 - ETA: 9s - loss: 0.6941 - acc: 0.5078 - ETA: 8s - loss: 0.6937 - acc: 0.509 - ETA: 7s - loss: 0.6941 - acc: 0.506 - ETA: 6s - loss: 0.6948 - acc: 0.500 - ETA: 5s - loss: 0.6946 - acc: 0.502 - ETA: 4s - loss: 0.6944 - acc: 0.509 - ETA: 3s - loss: 0.6942 - acc: 0.512 - ETA: 2s - loss: 0.6942 - acc: 0.509 - ETA: 1s - loss: 0.6941 - acc: 0.506 - ETA: 0s - loss: 0.6945 - acc: 0.495 - 25s 36ms/step - loss: 0.6945 - acc: 0.4963 - val_loss: 0.6924 - val_acc: 0.5342\n",
      "Epoch 2/10\n",
      "679/679 [==============================] - ETA: 18s - loss: 0.6900 - acc: 0.65 - ETA: 17s - loss: 0.6916 - acc: 0.54 - ETA: 16s - loss: 0.6916 - acc: 0.54 - ETA: 15s - loss: 0.6910 - acc: 0.57 - ETA: 14s - loss: 0.6914 - acc: 0.58 - ETA: 14s - loss: 0.6915 - acc: 0.57 - ETA: 13s - loss: 0.6909 - acc: 0.59 - ETA: 12s - loss: 0.6913 - acc: 0.57 - ETA: 11s - loss: 0.6907 - acc: 0.57 - ETA: 10s - loss: 0.6906 - acc: 0.57 - ETA: 9s - loss: 0.6893 - acc: 0.5852 - ETA: 8s - loss: 0.6885 - acc: 0.585 - ETA: 7s - loss: 0.6896 - acc: 0.574 - ETA: 6s - loss: 0.6902 - acc: 0.569 - ETA: 5s - loss: 0.6928 - acc: 0.556 - ETA: 4s - loss: 0.6945 - acc: 0.544 - ETA: 3s - loss: 0.6952 - acc: 0.534 - ETA: 2s - loss: 0.6947 - acc: 0.533 - ETA: 2s - loss: 0.6942 - acc: 0.542 - ETA: 1s - loss: 0.6940 - acc: 0.542 - ETA: 0s - loss: 0.6938 - acc: 0.550 - 22s 33ms/step - loss: 0.6938 - acc: 0.5479 - val_loss: 0.6917 - val_acc: 0.5753\n",
      "Epoch 3/10\n",
      "679/679 [==============================] - ETA: 18s - loss: 0.6895 - acc: 0.78 - ETA: 17s - loss: 0.6908 - acc: 0.65 - ETA: 16s - loss: 0.6907 - acc: 0.62 - ETA: 15s - loss: 0.6918 - acc: 0.57 - ETA: 14s - loss: 0.6911 - acc: 0.57 - ETA: 13s - loss: 0.6907 - acc: 0.58 - ETA: 13s - loss: 0.6906 - acc: 0.58 - ETA: 12s - loss: 0.6900 - acc: 0.58 - ETA: 11s - loss: 0.6891 - acc: 0.58 - ETA: 10s - loss: 0.6879 - acc: 0.58 - ETA: 9s - loss: 0.6880 - acc: 0.5767 - ETA: 8s - loss: 0.6876 - acc: 0.572 - ETA: 7s - loss: 0.6876 - acc: 0.567 - ETA: 6s - loss: 0.6883 - acc: 0.564 - ETA: 5s - loss: 0.6906 - acc: 0.550 - ETA: 4s - loss: 0.6904 - acc: 0.548 - ETA: 3s - loss: 0.6901 - acc: 0.551 - ETA: 2s - loss: 0.6901 - acc: 0.555 - ETA: 2s - loss: 0.6896 - acc: 0.567 - ETA: 1s - loss: 0.6891 - acc: 0.575 - ETA: 0s - loss: 0.6893 - acc: 0.568 - 23s 33ms/step - loss: 0.6891 - acc: 0.5714 - val_loss: 0.6897 - val_acc: 0.5342\n",
      "Epoch 4/10\n",
      "679/679 [==============================] - ETA: 19s - loss: 0.6811 - acc: 0.62 - ETA: 18s - loss: 0.6849 - acc: 0.54 - ETA: 17s - loss: 0.6845 - acc: 0.52 - ETA: 16s - loss: 0.6870 - acc: 0.52 - ETA: 15s - loss: 0.6854 - acc: 0.55 - ETA: 14s - loss: 0.6832 - acc: 0.56 - ETA: 14s - loss: 0.6842 - acc: 0.55 - ETA: 13s - loss: 0.6827 - acc: 0.55 - ETA: 11s - loss: 0.6806 - acc: 0.56 - ETA: 10s - loss: 0.6799 - acc: 0.56 - ETA: 9s - loss: 0.6801 - acc: 0.5625 - ETA: 8s - loss: 0.6782 - acc: 0.565 - ETA: 7s - loss: 0.6757 - acc: 0.562 - ETA: 6s - loss: 0.6801 - acc: 0.551 - ETA: 5s - loss: 0.6830 - acc: 0.547 - ETA: 4s - loss: 0.6825 - acc: 0.552 - ETA: 3s - loss: 0.6831 - acc: 0.553 - ETA: 3s - loss: 0.6815 - acc: 0.562 - ETA: 2s - loss: 0.6793 - acc: 0.570 - ETA: 1s - loss: 0.6797 - acc: 0.567 - ETA: 0s - loss: 0.6814 - acc: 0.559 - 22s 33ms/step - loss: 0.6811 - acc: 0.5596 - val_loss: 0.6837 - val_acc: 0.5274\n",
      "Epoch 5/10\n",
      "679/679 [==============================] - ETA: 18s - loss: 0.6845 - acc: 0.46 - ETA: 17s - loss: 0.6804 - acc: 0.56 - ETA: 17s - loss: 0.6767 - acc: 0.55 - ETA: 16s - loss: 0.6698 - acc: 0.57 - ETA: 15s - loss: 0.6693 - acc: 0.55 - ETA: 14s - loss: 0.6658 - acc: 0.57 - ETA: 13s - loss: 0.6674 - acc: 0.58 - ETA: 12s - loss: 0.6651 - acc: 0.60 - ETA: 11s - loss: 0.6608 - acc: 0.62 - ETA: 10s - loss: 0.6579 - acc: 0.62 - ETA: 9s - loss: 0.6537 - acc: 0.6335 - ETA: 8s - loss: 0.6524 - acc: 0.630 - ETA: 7s - loss: 0.6651 - acc: 0.615 - ETA: 6s - loss: 0.6604 - acc: 0.620 - ETA: 5s - loss: 0.6612 - acc: 0.614 - ETA: 4s - loss: 0.6593 - acc: 0.617 - ETA: 3s - loss: 0.6633 - acc: 0.606 - ETA: 2s - loss: 0.6634 - acc: 0.602 - ETA: 2s - loss: 0.6662 - acc: 0.598 - ETA: 1s - loss: 0.6662 - acc: 0.600 - ETA: 0s - loss: 0.6673 - acc: 0.596 - 23s 33ms/step - loss: 0.6679 - acc: 0.5950 - val_loss: 0.6669 - val_acc: 0.5856\n",
      "Epoch 6/10\n",
      "679/679 [==============================] - ETA: 17s - loss: 0.6543 - acc: 0.68 - ETA: 17s - loss: 0.6636 - acc: 0.67 - ETA: 16s - loss: 0.6525 - acc: 0.67 - ETA: 15s - loss: 0.6443 - acc: 0.67 - ETA: 14s - loss: 0.6436 - acc: 0.66 - ETA: 13s - loss: 0.6458 - acc: 0.65 - ETA: 12s - loss: 0.6525 - acc: 0.62 - ETA: 12s - loss: 0.6466 - acc: 0.63 - ETA: 11s - loss: 0.6462 - acc: 0.63 - ETA: 10s - loss: 0.6490 - acc: 0.62 - ETA: 9s - loss: 0.6498 - acc: 0.6222 - ETA: 8s - loss: 0.6496 - acc: 0.627 - ETA: 7s - loss: 0.6493 - acc: 0.629 - ETA: 6s - loss: 0.6497 - acc: 0.625 - ETA: 5s - loss: 0.6461 - acc: 0.637 - ETA: 5s - loss: 0.6476 - acc: 0.628 - ETA: 4s - loss: 0.6430 - acc: 0.639 - ETA: 3s - loss: 0.6395 - acc: 0.647 - ETA: 2s - loss: 0.6388 - acc: 0.651 - ETA: 1s - loss: 0.6414 - acc: 0.648 - ETA: 0s - loss: 0.6364 - acc: 0.654 - 23s 34ms/step - loss: 0.6356 - acc: 0.6554 - val_loss: 0.7024 - val_acc: 0.5788\n",
      "Epoch 7/10\n",
      "679/679 [==============================] - ETA: 18s - loss: 0.6354 - acc: 0.59 - ETA: 17s - loss: 0.6761 - acc: 0.53 - ETA: 16s - loss: 0.6542 - acc: 0.56 - ETA: 15s - loss: 0.6497 - acc: 0.57 - ETA: 14s - loss: 0.6237 - acc: 0.63 - ETA: 13s - loss: 0.6337 - acc: 0.60 - ETA: 13s - loss: 0.6372 - acc: 0.60 - ETA: 12s - loss: 0.6329 - acc: 0.60 - ETA: 11s - loss: 0.6299 - acc: 0.60 - ETA: 10s - loss: 0.6208 - acc: 0.61 - ETA: 9s - loss: 0.6241 - acc: 0.6222 - ETA: 8s - loss: 0.6192 - acc: 0.635 - ETA: 7s - loss: 0.6136 - acc: 0.644 - ETA: 6s - loss: 0.6065 - acc: 0.649 - ETA: 5s - loss: 0.6124 - acc: 0.643 - ETA: 4s - loss: 0.6137 - acc: 0.640 - ETA: 3s - loss: 0.6143 - acc: 0.643 - ETA: 3s - loss: 0.6178 - acc: 0.640 - ETA: 2s - loss: 0.6153 - acc: 0.643 - ETA: 1s - loss: 0.6156 - acc: 0.645 - ETA: 0s - loss: 0.6150 - acc: 0.647 - 22s 33ms/step - loss: 0.6154 - acc: 0.6465 - val_loss: 0.6725 - val_acc: 0.6096\n",
      "Epoch 8/10\n",
      "679/679 [==============================] - ETA: 19s - loss: 0.5791 - acc: 0.68 - ETA: 18s - loss: 0.5750 - acc: 0.73 - ETA: 17s - loss: 0.5729 - acc: 0.69 - ETA: 16s - loss: 0.5680 - acc: 0.70 - ETA: 15s - loss: 0.5851 - acc: 0.68 - ETA: 14s - loss: 0.6031 - acc: 0.66 - ETA: 13s - loss: 0.5949 - acc: 0.69 - ETA: 12s - loss: 0.5985 - acc: 0.68 - ETA: 11s - loss: 0.6032 - acc: 0.67 - ETA: 10s - loss: 0.6013 - acc: 0.68 - ETA: 9s - loss: 0.6030 - acc: 0.6790 - ETA: 8s - loss: 0.5942 - acc: 0.679 - ETA: 7s - loss: 0.5930 - acc: 0.680 - ETA: 6s - loss: 0.5950 - acc: 0.678 - ETA: 5s - loss: 0.5935 - acc: 0.677 - ETA: 4s - loss: 0.6006 - acc: 0.669 - ETA: 3s - loss: 0.5951 - acc: 0.676 - ETA: 2s - loss: 0.5962 - acc: 0.677 - ETA: 2s - loss: 0.5901 - acc: 0.684 - ETA: 1s - loss: 0.5939 - acc: 0.685 - ETA: 0s - loss: 0.5895 - acc: 0.694 - 22s 33ms/step - loss: 0.5884 - acc: 0.6966 - val_loss: 0.6729 - val_acc: 0.5890\n",
      "Epoch 9/10\n",
      "679/679 [==============================] - ETA: 18s - loss: 0.6199 - acc: 0.68 - ETA: 17s - loss: 0.5757 - acc: 0.70 - ETA: 17s - loss: 0.5692 - acc: 0.69 - ETA: 16s - loss: 0.5432 - acc: 0.72 - ETA: 15s - loss: 0.5559 - acc: 0.72 - ETA: 15s - loss: 0.5421 - acc: 0.73 - ETA: 14s - loss: 0.5537 - acc: 0.72 - ETA: 13s - loss: 0.5707 - acc: 0.71 - ETA: 12s - loss: 0.5685 - acc: 0.70 - ETA: 11s - loss: 0.5645 - acc: 0.70 - ETA: 10s - loss: 0.5668 - acc: 0.70 - ETA: 8s - loss: 0.5601 - acc: 0.7109 - ETA: 8s - loss: 0.5544 - acc: 0.716 - ETA: 7s - loss: 0.5521 - acc: 0.714 - ETA: 6s - loss: 0.5495 - acc: 0.716 - ETA: 5s - loss: 0.5581 - acc: 0.712 - ETA: 4s - loss: 0.5554 - acc: 0.715 - ETA: 3s - loss: 0.5512 - acc: 0.718 - ETA: 2s - loss: 0.5493 - acc: 0.722 - ETA: 1s - loss: 0.5459 - acc: 0.728 - ETA: 0s - loss: 0.5430 - acc: 0.733 - 23s 34ms/step - loss: 0.5423 - acc: 0.7349 - val_loss: 0.6529 - val_acc: 0.6336\n",
      "Epoch 10/10\n",
      "679/679 [==============================] - ETA: 18s - loss: 0.4309 - acc: 0.81 - ETA: 17s - loss: 0.4566 - acc: 0.82 - ETA: 16s - loss: 0.4694 - acc: 0.81 - ETA: 15s - loss: 0.4534 - acc: 0.81 - ETA: 14s - loss: 0.4479 - acc: 0.81 - ETA: 13s - loss: 0.4408 - acc: 0.81 - ETA: 12s - loss: 0.4619 - acc: 0.78 - ETA: 12s - loss: 0.4684 - acc: 0.78 - ETA: 11s - loss: 0.4697 - acc: 0.78 - ETA: 10s - loss: 0.4764 - acc: 0.77 - ETA: 9s - loss: 0.5057 - acc: 0.7443 - ETA: 8s - loss: 0.5123 - acc: 0.731 - ETA: 7s - loss: 0.5086 - acc: 0.735 - ETA: 6s - loss: 0.5053 - acc: 0.736 - ETA: 5s - loss: 0.5161 - acc: 0.727 - ETA: 4s - loss: 0.5161 - acc: 0.722 - ETA: 3s - loss: 0.5140 - acc: 0.722 - ETA: 2s - loss: 0.5159 - acc: 0.720 - ETA: 2s - loss: 0.5225 - acc: 0.715 - ETA: 1s - loss: 0.5202 - acc: 0.723 - ETA: 0s - loss: 0.5199 - acc: 0.723 - 22s 33ms/step - loss: 0.5205 - acc: 0.7246 - val_loss: 0.6773 - val_acc: 0.6473\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# more info on callbakcs: https://keras.io/callbacks/ model saver is cool too.\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "dense_layers = [0]\n",
    "layer_sizes = [64]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=32,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])\n",
    "\n",
    "model.save('64x3-CNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "Cat0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "CATEGORIES = [\"Dog0\", \"Cat0\"]\n",
    "\n",
    "\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 100  # 50 in txt-based\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(\"64x3-CNN.model\")\n",
    "\n",
    "prediction = model.predict([prepare('9909.jpg')])\n",
    "print(prediction)  # will be a list in a list.\n",
    "print(CATEGORIES[int(prediction[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
